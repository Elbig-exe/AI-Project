# -*- coding: utf-8 -*-
"""projet IA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L5BHF2Z3FB9x6u2EF1uImdjxH2eebhkj

# Phase 1:

analyse exploratoire:
"""

import pandas as pd
data=pd.read_csv("/content/Phase1.csv") 
data.head()

""" phase exploratoire"""

data.describe()

del data["Unnamed: 0"] #supprimer la colonne du ID
data.head()

"""Data.isnull nous donne une table boolean qui signifie l'existence de la valeur , et sum fait la somme 
 de tout les row d'apres le resultat , pas de cellule null
"""

data.isnull().sum()

"""les colonne ou la variance =0"""

var=data.var().gt(0.0)
for i in range(0,var.size):
  if(not var[i]):
    print(var.index[i])

"""Eliminer les colonne avec une variance =0"""

for i in range(0,var.size):
  if(not var[i]):
    del data[var.index[i]]

"""Supprimer les redondances"""

data_clean=data.drop_duplicates()
print(data_clean.shape[0]-data.shape[0])

"""Pas des redondances

La classe normal est surreprésenter dans le dataset
"""

data['class'].value_counts().plot.bar()

target=data["class"]
data= data.iloc[:,:-1]

data=pd.get_dummies(data) 
data.head()

from sklearn.preprocessing import scale
data=scale(data)

"""## Application knn:

Decouper le dataset
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3,stratify=target)#random_state pour avoire le meme resultat

"""Recherche dea meilleur parametres"""

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
grid_params={
    'n_neighbors':[3,5,7,9,11],
    'metric':['euclidean','manhattan']
}

gs=GridSearchCV(
   KNeighborsClassifier(),
  grid_params,
    cv=5
)
gs_results=gs.fit(X_train, y_train)
gs_results.best_params_

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=3,metric='manhattan')
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

"""## Fonction table de confusion"""

#y_p :la prediction
#y_t :y de test
#positive: la classe positive
def matrice_conf(y_p,y_t,positive=None):
  y_t=y_t.array
  if positive==None:
    positive=y_t[0]
  if(y_p.size!=y_t.size): 
    raise ValueError('le nombre de prediction est different de nombre de test')
  result=[[0,0],
          [0,0]]
  for i in range(0,y_t.size):
    if y_p[i]==y_t[i] and y_p[i]==positive:
      result[0][0]+=1
    elif (y_p[i]==y_t[i]):
      result[1][1]+=1
    elif (y_p[i]==positive):
      result[1][0]+=1
    else:
      result[0][1]+=1
  return result

"""## 1.4

Import et initialisation du kfold
"""

from sklearn.model_selection import KFold
kfold=KFold(5,shuffle=True,random_state=1)

"""Creation d'une instance de knn avec les parametres optimales et application de cross validation avec training data"""

classifier = KNeighborsClassifier(n_neighbors=3,metric='manhattan')
rap_dis=[]
i=0
for train_index, test_index in kfold.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]
    classifier.fit(X_train_fold, y_train_fold)
    y_pred_fold = classifier.predict(X_test_fold)
    mat=matrice_conf(y_pred_fold,y_test_fold,"anomaly")
    rap_dis.append([])
    rap_dis[i].append(mat[0][0]/(mat[0][0]+mat[1][0]))
    rap_dis[i].append(mat[0][0]/(mat[0][0]+mat[0][1]))
    i+=1

"""Faire une prediction avec les 30% de test data(non deja utiliser avec le modele ) """

y_pred = classifier.predict(X_test)

"""Les calcules"""

mat=matrice_conf(y_pred,y_test,positive="anomaly") #matrice de confusion 
rappel=mat[0][0]/(mat[0][0]+mat[0][1])
precision=mat[0][0]/(mat[0][0]+mat[1][0])
print("precision=",precision)
print("rappel=",rappel)
print("F1=",(2*precision*rappel)/(precision+rappel))

print(rap_dis)
f=[]
for i in range(0,5):
  f.append((2*rap_dis[i][0]*rap_dis[i][1])/(rap_dis[i][0]+rap_dis[i][1]))
print(f)

mean=0;
for i in range(0,5):
  mean+=f[i]
mean=mean/5;
print("moyenne de F1=",mean)

"""1.5: le modèle predicte bien les données de test, mais 
1.6:d'apres la visualisation de la colonne "class" ,les packets normale sont surrepresenter dans le dataset ,donc le dataset ne represente pas le probleme presenter

# Phase 2

## Phase exploratoire:
"""

data2=pd.read_csv("/content/Phase2.csv") 
data2.head()

"""Pas de colonne d'ID """

data2['class'].value_counts().plot.bar()

data2.isnull().sum()

var=data2.var().gt(0.0)
for i in range(0,var.size):
  if(not var[i]):
    print(var.index[i])

for i in range(0,var.size):
  if(not var[i]):
    del data2[var.index[i]]

data_clean=data2.drop_duplicates()
print(data_clean.shape[0]-data2.shape[0])

target2=data2["class"]
data2 = data2.iloc[:,:-1]

data2=pd.get_dummies(data2) 
data2.head()

data2=scale(data2)

"""## Les changements apportés aux données sont:
  Le nombre de packet normal et anomaly est equilibrer 
  une augmentation dans le nombres des donnee dans le dataset

## Application knn:
"""

#random_state pour avoire le meme resultat 
X_train2, X_test2, y_train2, y_test2 = train_test_split(data2, target2, test_size=0.3,stratify=target2)

classifier = KNeighborsClassifier(n_neighbors=3,metric='manhattan')

kfold=KFold(5,random_state=1)
rap_dis=[]#rapport /dis
i=0
for train_index, test_index in kfold.split(X_train2):
    X_train_fold, X_test_fold = X_train2[train_index], X_train2[test_index]
    y_train_fold, y_test_fold = y_train2.iloc[train_index], y_train2.iloc[test_index]
    classifier.fit(X_train_fold, y_train_fold)
    y_pred_fold = classifier.predict(X_test_fold)
    mat=matrice_conf(y_pred_fold,y_test_fold,"anomaly")
    rap_dis.append([])
    rap_dis[i].append(mat[0][0]/(mat[0][0]+mat[1][0]))
    rap_dis[i].append(mat[0][0]/(mat[0][0]+mat[0][1]))
    i+=1

y_pred2 = classifier.predict(X_test2)

mat=matrice_conf(y_pred2,y_test2,positive="anomaly") #matrice de confusion 
rappel=mat[0][0]/(mat[0][0]+mat[0][1])
precision=mat[0][0]/(mat[0][0]+mat[1][0])
print("precision=",precision)
print("rappel=",rappel)
print("F1=",(2*precision*rappel)/(precision+rappel))

print(rap_dis)
f=[]
for i in range(0,5):
  f.append((2*rap_dis[i][0]*rap_dis[i][1])/(rap_dis[i][0]+rap_dis[i][1]))
print(f)

mean=0;
for i in range(0,5):
  mean+=f[i]
mean=mean/5;
print(mean)

"""## L'impact des changement sur les performances :
  oui les changements ont un impact : 
  

* l'equilibre entre les packet normal et anomaly donne une bonne representation du problem :le f1 score de phase 2 est 0.993 parraport au f1 score de phase1 0.918

#Phase 3
"""

import pandas as pd
data3=pd.read_csv("/content/Phase3.csv") 
data3.head()

"""Supprimer les ligne avec des valeur none """

data3=data3.dropna()

"""This is formatted as code



"""

del data3["protocol_type"]
del data3["service"]
del data3["flag"]
del data3["logged_in"]

data3.describe()

#importe de silhouette pour mesurer la performances du classificateur
from sklearn.metrics import silhouette_score

"""Ce bloc recherche les colonnes avec un pourcentage à enlever de la méme valeur      

*  Supprime ces colonnes.
*  Normaliser le dataset.
*   Application d'algorithme kmeans.
*   Mettre la silhouette de chaque i dans le vecteur "sil".
Objctive: trouver et supprimer les colonnes d'une facon pour maximizer la silhouette.




"""

sil=[] 
for i in range(70,95):
  data=data3.copy()
  num_rows = len(data3.index)
  low_information_cols = [] #
  for col in data3.columns:
     cnts = data3[col].value_counts(dropna=False)
     top_pct = (cnts/num_rows).iloc[0]
     if top_pct > i/100:
         low_information_cols.append(col)
  for info in low_information_cols:
    del data[info]
  data = scale(data)
  kmeans= KMeans(n_clusters=2)
  y = kmeans.fit_predict(data)
  sil.append(silhouette_score(data, y))

"""Afficher les sillhouette """

from matplotlib import pyplot as plt
plt.plot(sil)

"""Le meilleur pourcentage est 76%"""

num_rows = len(data3.index)
low_information_cols = [] #
for col in data3.columns:
    cnts = data3[col].value_counts(dropna=False)
    top_pct = (cnts/num_rows).iloc[0]
    if top_pct > 0.76:
        low_information_cols.append(col)

for info in low_information_cols:
  del data3[info]

#data3 =data3.drop_duplicates()

from sklearn.preprocessing import scale 
data3_scaled = scale(data3)

"""training de kmeans avec le dataset transformer """

from sklearn.cluster import KMeans
kmeans= KMeans(n_clusters=2)
y = kmeans.fit_predict(data3_scaled)
print(silhouette_score(data3_scaled, y))

"""## 3.3"""

data3_labled=pd.read_csv("/content/Phase3.csv") 
data3=data3.dropna()

labels=[]
for i in range(0,kmeans.labels_.shape[0]):
  if(kmeans.labels_[i]==1):
    labels.append("normal")
  else:
    labels.append("anomaly")

data3_labled['class'] = labels

data3_labled.head()

data2=pd.read_csv("/content/Phase2.csv")

data2.shape

"""mixage de data "Phase2,Phase3""""

data =data2.append(data3_labled, ignore_index=True)

"""affiche de nouveaux data"""

data['class'].value_counts().plot.bar()

data2.isnull().sum()

"""supprimer les lignes avec des valeur none"""

data=data.dropna()

"""Supprimer les redondances:"""

data_clean=data.drop_duplicates()
print(data_clean.shape[0]-data2.shape[0])

target=data_clean["class"]
data_clean = data_clean.iloc[:,:-1]

data_clean=pd.get_dummies(data_clean) 
data_clean.head()

data_clean=scale(data_clean)

"""## Application knn 3.3:"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data_clean, target, test_size=0.3,stratify=target)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=3,metric='manhattan')
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

"""Table de confusion"""

mat=matrice_conf(y_pred,y_test,positive="anomaly") #matrice de confusion

rappel=mat[0][0]/(mat[0][0]+mat[0][1])
precision=mat[0][0]/(mat[0][0]+mat[1][0])
print("precision=",precision)
print("rappel=",rappel)
print("F1=",(2*precision*rappel)/(precision+rappel))